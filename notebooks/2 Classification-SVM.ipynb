{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauro/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mauro/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mauro/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mauro/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mauro/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/mauro/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mauro/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold  # , cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support, confusion_matrix  # , classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy import integrate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/features/'\n",
    "\n",
    "tran = pd.read_csv(DATA_PATH + \"T.csv\")\n",
    "tran = tran.set_index(['ID', 'copy_num'])\n",
    "\n",
    "ntran = pd.read_csv(DATA_PATH + \"NT.csv\")\n",
    "ntran = ntran.set_index(['ID', 'copy_num'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amout of obects (oversampled):     62152\n",
      "Total amout of obects (not oversampled): 19462\n"
     ]
    }
   ],
   "source": [
    "print(\"Total amout of obects (oversampled):     {}\".format(tran.shape[0]+ntran.shape[0]))\n",
    "print(\"Total amout of obects (not oversampled): {}\".format(tran[tran.index.get_level_values(\"copy_num\")==0].shape[0]+ntran.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 30\n",
      "\n",
      "The features are:\n",
      "    1. amplitude\n",
      "    2. beyond1st\n",
      "    3. flux_percentile_ratio_mid20\n",
      "    4. flux_percentile_ratio_mid35\n",
      "    5. flux_percentile_ratio_mid50\n",
      "    6. flux_percentile_ratio_mid65\n",
      "    7. flux_percentile_ratio_mid80\n",
      "    8. kurtosis\n",
      "    9. max_slope\n",
      "    10. median_absolute_deviation\n",
      "    11. median_buffer_range_percentage\n",
      "    12. pair_slope_trend\n",
      "    13. pair_slope_trend_last_30\n",
      "    14. percent_amplitude\n",
      "    15. percent_difference_flux_percentile\n",
      "    16. poly1_t1\n",
      "    17. poly2_t1\n",
      "    18. poly2_t2\n",
      "    19. poly3_t1\n",
      "    20. poly3_t2\n",
      "    21. poly3_t3\n",
      "    22. poly4_t1\n",
      "    23. poly4_t2\n",
      "    24. poly4_t3\n",
      "    25. poly4_t4\n",
      "    26. skew\n",
      "    27. small_kurtosis\n",
      "    28. std\n",
      "    29. stetson_j\n",
      "    30. stetson_k\n"
     ]
    }
   ],
   "source": [
    "feats = np.array(tran.columns[2:])\n",
    "print(\"Total number of features: {}\".format(len(feats)))\n",
    "print()\n",
    "print(\"The features are:\")\n",
    "for i,f in enumerate(feats):\n",
    "    print(\"    \"+str(i+1)+\". \" + f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf = pd.concat([tran,ntran])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification \n",
    "Transients and non-transients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(dataframe):\n",
    "    #create output dataframes\n",
    "    test = pd.DataFrame(columns = [\"ID\",\"copy_num\"]+list(dataframe.columns))\n",
    "    test = test.set_index([\"ID\",\"copy_num\"])\n",
    "\n",
    "    train = pd.DataFrame(columns = [\"ID\",\"copy_num\"]+list(dataframe.columns))\n",
    "    train = train.set_index([\"ID\",\"copy_num\"])\n",
    "    \n",
    "    for uClass in dataframe.Class.unique():\n",
    "        #get each class \n",
    "        classDf = dataframe[dataframe.Class == uClass]\n",
    "        \n",
    "        #unique ids\n",
    "        ids = classDf.index.get_level_values('ID').unique()\n",
    "\n",
    "        # randomly choose 25% of indices \n",
    "\n",
    "        testInd = np.random.choice(ids, int(0.25*len(ids)),replace=False)\n",
    "\n",
    "        #get dataframes\n",
    "        test = pd.concat([test,classDf[classDf.index.get_level_values('ID').isin(testInd)]])\n",
    "        \n",
    "        train = pd.concat([train,classDf[~classDf.index.get_level_values('ID').isin(testInd)]])\n",
    "\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(df):\n",
    "    #start min at infinity \n",
    "    minNum = np.inf\n",
    "    \n",
    "    #find the class with the minimum amount of candidates\n",
    "    for classElem in df.Class.unique():\n",
    "        numElems = len(df[df.Class==classElem])\n",
    "        if(numElems<minNum):\n",
    "            minNum=numElems\n",
    "    \n",
    "    #create output dataframe\n",
    "    ans = pd.DataFrame(columns = [\"ID\",\"copy_num\"]+list(df.columns))\n",
    "    ans = ans.set_index([\"ID\",\"copy_num\"])\n",
    "    \n",
    "    #get a sample from all the classes \n",
    "    for classElem in df.Class.unique():\n",
    "        ans = pd.concat([ans,df[df.Class==classElem].sample(n=minNum)])\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypermarameters that gridsearch will optimize\n",
    "def rf():\n",
    "    params = {\n",
    "        'kernel': ['linear','poly', 'rbf', 'sigmoid'],\n",
    "        'C': [2**x for x in range(-3,6,4) ],\n",
    "        'gamma': [2**x for x in range(-3,6,4) ]\n",
    "    }\n",
    "    return SVC(random_state=0, class_weight='balanced'), params\n",
    "\n",
    "#metrics to be analized\n",
    "def scorers():\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "               'precision': make_scorer(precision_score, average='weighted'),\n",
    "               'recall': make_scorer(recall_score, average='weighted'),\n",
    "               'f1_score': make_scorer(f1_score, average='weighted')\n",
    "               }\n",
    "    return scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy and shuffle the data\n",
    "oversampling = alldf.copy()\n",
    "oversampling = oversampling.sample(frac=1)\n",
    "\n",
    "#map all objects that are not non-transient to transient\n",
    "oversampling.Class = list(map(lambda x: 'non-transient' if x=='non-transient' else 'transient', oversampling.Class))\n",
    "#map transient and non-transient to binary values\n",
    "oversampling['target'] = list(map(lambda x: 1 if x=='transient' else 0, oversampling.Class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n",
      "[CV] C=0.125, gamma=0.125, kernel=linear .............................\n"
     ]
    }
   ],
   "source": [
    "recall_scores = []\n",
    "\n",
    "    \n",
    "#split train test class by class\n",
    "all_train,all_test = splitTrainTest(oversampling)\n",
    "\n",
    "#balance the train set\n",
    "all_train= balance(all_train)\n",
    "\n",
    "\n",
    "\n",
    "#train indices\n",
    "trainIdx = all_train.index.get_level_values(\"ID\").unique()\n",
    "\n",
    "#remove originals that have oversampled copies in train\n",
    "all_test = all_test[~all_test.index.get_level_values(\"ID\").isin(trainIdx)]\n",
    "\n",
    "#remove oversampled data from test set\n",
    "all_test = all_test[all_test.index.get_level_values('copy_num') ==0 ]\n",
    "\n",
    "\n",
    "#format target variable to appropriate data type\n",
    "all_train.target= all_train.target.astype('int')\n",
    "all_test.target= all_test.target.astype('int')\n",
    "\n",
    "#learning \n",
    "model,params = rf()\n",
    "grid_search = GridSearchCV(model, params, cv=StratifiedKFold(2), scoring=scorers(),\n",
    "                           refit='f1_score', return_train_score=True,verbose=100,n_jobs=1)\n",
    "grid_search.fit(all_train[feats], all_train.target)\n",
    "\n",
    "# Copy classifier \n",
    "clf = grid_search\n",
    "\n",
    "#see performance on test set\n",
    "scores = precision_recall_fscore_support(\n",
    "        all_test.target, clf.predict(all_test[feats]), average='weighted')\n",
    "recall_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scoresnp = np.array(recall_scores)\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(np.mean(recall_scoresnp[:,0])))\n",
    "print(\"Recall:    {:.4f}\".format(np.mean(recall_scoresnp[:,1])))\n",
    "print(\"F-score:   {:.4f}\".format(np.mean(recall_scoresnp[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3549,  216],\n",
       "       [ 249,  851]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(all_test.target, clf.predict(all_test[feats])).transpose()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fMeasure(precision,recall):\n",
    "    return 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisionNon = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "recallNon = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "precisionT = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "recallT = cm[1][1]/(cm[1][1]+cm[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of transients:     0.7736\n",
      "Recall of transients:        0.7976\n",
      "F-measure of transients:     0.7854\n",
      "Precision of non-transients: 0.9426\n",
      "Recall of non-transients:    0.9344\n",
      "F-measure of non-transients: 0.9385\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision of transients:     {:.4f}\".format(precisionT))\n",
    "print(\"Recall of transients:        {:.4f}\".format(recallT))\n",
    "print(\"F-measure of transients:     {:.4f}\".format(fMeasure(precisionT,recallT)))\n",
    "print(\"Precision of non-transients: {:.4f}\".format(precisionNon))\n",
    "print(\"Recall of non-transients:    {:.4f}\".format(recallNon))\n",
    "print(\"F-measure of non-transients: {:.4f}\".format(fMeasure(precisionNon,recallNon)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-Class clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main 6 transient classes\n",
    "labels = ['SN', 'CV', 'AGN', 'HPM', 'Blazar', 'Flare']\n",
    "\n",
    "#copy transient data\n",
    "tran8 = tran.copy()\n",
    "\n",
    "#if class is not in labels, label as other\n",
    "tran8.loc[~tran8.Class.isin(labels), ['Class']] = 'Other'\n",
    "\n",
    "#copy non transient\n",
    "ntran8 = ntran.copy()\n",
    "\n",
    "#transients and non-transients\n",
    "all8 = pd.concat([tran8,ntran8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to map labels to integer values\n",
    "def manualFact(lab):\n",
    "    labels = ['SN', 'CV', 'AGN', 'HPM', 'Blazar', 'Flare','Other','non-transient']\n",
    "    return labels.index(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels to integer values\n",
    "all8['target'] = list(map(lambda x: manualFact(x),all8['Class']))\n",
    "\n",
    "# copy labeled data\n",
    "oversampling = all8.copy()\n",
    "\n",
    "#shuffle data\n",
    "oversampling = oversampling.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n",
      "[CV]  max_features=auto, n_estimators=200, accuracy=0.7875874125874126, precision=0.7854990399005988, recall=0.7875874125874126, f1_score=0.7857555771663376, total=   6.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.1s remaining:    0.0s\n",
      "[CV] max_features=auto, n_estimators=200 .............................\n",
      "[CV]  max_features=auto, n_estimators=200, accuracy=0.7687937062937062, precision=0.768041241692728, recall=0.7687937062937062, f1_score=0.7669391258180086, total=   5.6s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   13.7s remaining:    0.0s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV]  max_features=auto, n_estimators=700, accuracy=0.7881701631701632, precision=0.7860371397117597, recall=0.7881701631701632, f1_score=0.7864546609720969, total=  18.8s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   35.5s remaining:    0.0s\n",
      "[CV] max_features=auto, n_estimators=700 .............................\n",
      "[CV]  max_features=auto, n_estimators=700, accuracy=0.7738927738927739, precision=0.773778193591634, recall=0.7738927738927739, f1_score=0.7721963137471567, total=  21.8s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV]  max_features=sqrt, n_estimators=200, accuracy=0.7875874125874126, precision=0.7854990399005988, recall=0.7875874125874126, f1_score=0.7857555771663376, total=   5.6s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] max_features=sqrt, n_estimators=200 .............................\n",
      "[CV]  max_features=sqrt, n_estimators=200, accuracy=0.7687937062937062, precision=0.768041241692728, recall=0.7687937062937062, f1_score=0.7669391258180086, total=   6.7s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV]  max_features=sqrt, n_estimators=700, accuracy=0.7881701631701632, precision=0.7860371397117597, recall=0.7881701631701632, f1_score=0.7864546609720969, total=  20.8s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] max_features=sqrt, n_estimators=700 .............................\n",
      "[CV]  max_features=sqrt, n_estimators=700, accuracy=0.7738927738927739, precision=0.773778193591634, recall=0.7738927738927739, f1_score=0.7721963137471567, total=  18.7s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV]  max_features=log2, n_estimators=200, accuracy=0.7823426573426573, precision=0.7800746193141302, recall=0.7823426573426573, f1_score=0.7805127141541333, total=   6.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] max_features=log2, n_estimators=200 .............................\n",
      "[CV]  max_features=log2, n_estimators=200, accuracy=0.771416083916084, precision=0.7704571799585195, recall=0.771416083916084, f1_score=0.7692966212681135, total=   6.1s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV]  max_features=log2, n_estimators=700, accuracy=0.7827797202797203, precision=0.7807059558991926, recall=0.7827797202797203, f1_score=0.7809965345354671, total=  21.1s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  2.7min remaining:    0.0s\n",
      "[CV] max_features=log2, n_estimators=700 .............................\n",
      "[CV]  max_features=log2, n_estimators=700, accuracy=0.7695221445221445, precision=0.7691462845150481, recall=0.7695221445221445, f1_score=0.7677739170484553, total=  18.4s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  3.0min finished\n"
     ]
    }
   ],
   "source": [
    "recall_scores = []\n",
    "\n",
    "#split train test class by class\n",
    "all_train,all_test = splitTrainTest(oversampling)\n",
    "\n",
    "#balance the train set\n",
    "all_train= balance(all_train)\n",
    "\n",
    "\n",
    "\n",
    "#train indices\n",
    "trainIdx = all_train.index.get_level_values(\"ID\").unique()\n",
    "\n",
    "#remove originals that may have oversampled copies in train\n",
    "all_test = all_test[~all_test.index.get_level_values(\"ID\").isin(trainIdx)]\n",
    "\n",
    "#remove oversampling in test\n",
    "all_test = all_test[all_test.index.get_level_values('copy_num')==0]\n",
    "\n",
    "\n",
    "#format stuff\n",
    "all_train.target= all_train.target.astype('int')\n",
    "all_test.target= all_test.target.astype('int')\n",
    "\n",
    "model,params = rf()\n",
    "grid_search = GridSearchCV(model, params, cv=StratifiedKFold(2), scoring=scorers(),\n",
    "                           refit='f1_score', return_train_score=True,verbose=100)\n",
    "grid_search.fit(all_train[feats], all_train.target)\n",
    "# Copy classifier\n",
    "clf = grid_search\n",
    "\n",
    "scores = precision_recall_fscore_support(\n",
    "        all_test.target, clf.predict(all_test[feats]), average='weighted')\n",
    "\n",
    "\n",
    "recall_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8404\n",
      "Recall:    0.7408\n",
      "F-score:   0.7790\n"
     ]
    }
   ],
   "source": [
    "recall_scoresnp = np.array(recall_scores)\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(np.mean(recall_scoresnp[:,0])))\n",
    "print(\"Recall:    {:.4f}\".format(np.mean(recall_scoresnp[:,1])))\n",
    "print(\"F-score:   {:.4f}\".format(np.mean(recall_scoresnp[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 169   25    0    1   13    3   40   78]\n",
      " [  28  146    0    1    7    1   18   49]\n",
      " [   7    3   84    0    9    1   32   35]\n",
      " [   4    2    2   66    0    1    1  186]\n",
      " [   8   10    6    0   25    0   14    6]\n",
      " [   9    9    1    0    1   25    9  279]\n",
      " [  48   14    3    1    3    1   80  158]\n",
      " [  50    6   10    7    1   19   40 3007]]\n"
     ]
    }
   ],
   "source": [
    "confMatr = confusion_matrix(all_test.target, clf.predict(all_test[feats])).transpose()\n",
    "print(confMatr)\n",
    "# 'SN', 'CV', 'AGN', 'HPM', 'Blazar', 'Flare','Other','non-transient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "normedMatrix = confMatr.copy()\n",
    "# print(normedMatrix)\n",
    "for i in range(len(normedMatrix)):\n",
    "#     print(normedMatrix[:,i]/normedMatrix[:,i].sum())\n",
    "    normedMatrix[:,i] = normedMatrix[:,i]/normedMatrix[:,i].sum()\n",
    "#     print(normedMatrix[:,i])\n",
    "# np.set_printoptions(suppress=True)\n",
    "print(normedMatrix)\n",
    "# np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "newMatr = []\n",
    "for i in range(len(confMatr)):\n",
    "    prec = confMatr[i][i]/(sum(confMatr[i,:]))\n",
    "    rec = confMatr[i][i]/(sum(confMatr[:,i]))\n",
    "    newMatr.append([prec,rec,fMeasure(prec,rec),sum(confMatr[:,i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision        Recall         F-score      Cover\n",
      "[[   0.51367781    0.52321981    0.51840491  323.        ]\n",
      " [   0.584         0.67906977    0.62795699  215.        ]\n",
      " [   0.49122807    0.79245283    0.60649819  106.        ]\n",
      " [   0.2519084     0.86842105    0.39053254   76.        ]\n",
      " [   0.36231884    0.42372881    0.390625     59.        ]\n",
      " [   0.07507508    0.49019608    0.13020833   51.        ]\n",
      " [   0.25974026    0.34188034    0.29520295  234.        ]\n",
      " [   0.95764331    0.79173249    0.86682041 3798.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print('    Precision        Recall         F-score      Cover')\n",
    "print(np.array(newMatr))\n",
    "# np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
